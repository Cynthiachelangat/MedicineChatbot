{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Development of a Medical Chatbot Using Natural Language Processing**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "\n",
    "In recent years, the integration of technology into healthcare has revolutionized how individuals access medical information and services. The increasing reliance on digital solutions has led to the development of medical chatbots, which are designed to provide instant, personalized responses to health-related inquiries. These AI-driven systems leverage Natural Language Processing (NLP) to understand user queries and deliver accurate, relevant information, effectively bridging the gap between medical expertise and patient needs.\n",
    "\n",
    "The importance of reliable medical information cannot be overstated, particularly in an era marked by the rapid spread of misinformation. Many people turn to the internet as their first source of information regarding health issues, yet navigating through the vast sea of available data can often be overwhelming and confusing. Medical chatbots have the potential to serve as trusted resources, offering users immediate access to accurate health information while guiding them through their questions in an understandable manner.\n",
    "\n",
    "The ultimate goal of this chatbot is to create a user-friendly interface that allows individuals to engage with the system seamlessly. Users will receive instant responses to their health-related inquiries, empowering them to make informed decisions about their health and wellbeing. By prioritizing clarity and reliability, this chatbot aspires to improve healthcare accessibility and literacy, particularly for those who may find traditional medical resources challenging to comprehend. This research not only highlights the significance of artificial intelligence in healthcare but also underscores the importance of using credible medical sources to ensure the accuracy and effectiveness of health-related technologies.\n",
    "\n",
    "\n",
    "## **Problem Statement**\n",
    "\n",
    "Despite the abundance of medical information available online, many individuals struggle to find accurate and easily understandable health resources. Traditional healthcare services are often burdened by high demand, leading to challenges in accessibility and timely information dissemination. This gap in available resources can result in individuals relying on unreliable sources or misinterpreting health-related information, which can adversely affect their decision-making and health outcomes.\n",
    "\n",
    "The primary goal of this project is to address these challenges by developing a medical chatbot that provides:\n",
    "\n",
    "- **Immediate Access:** Users will have instant access to medical information without the need to sift through multiple websites or sources.\n",
    "\n",
    "- **Clear Communication:** The chatbot will use simple language to explain medical terms and concepts, making information more accessible to the general public.\n",
    "\n",
    "- **Reliable Information:** By utilizing the Gale Encyclopedia of Medicine, the chatbot will offer trusted and verified medical knowledge, ensuring that users receive accurate guidance.\n",
    "\n",
    "## **Metrics of Success**\n",
    "\n",
    "To evaluate the effectiveness of the medical chatbot, the following metrics will be considered:\n",
    "\n",
    "- **User Satisfaction:** Measured through surveys assessing the clarity and usefulness of responses.\n",
    "\n",
    "- **Accuracy of Information:** The chatbot's responses will be compared to verified medical sources to ensure correctness.\n",
    "\n",
    "- **Response Time:** The average time taken by the chatbot to provide answers to user queries.\n",
    "\n",
    "- **Engagement Rate:** The frequency of user interactions and follow-up questions will indicate user interest and trust in the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Understanding**\n",
    "\n",
    "The Gale Encyclopedia of Medicine serves as the foundational data source for this project. This resource is well-respected in the medical community and contains comprehensive, peer-reviewed articles that cover a wide range of medical topics, including:\n",
    "\n",
    "- **Diseases and Conditions:** Detailed descriptions of various health conditions, their symptoms, causes, and treatment options.\n",
    "\n",
    "- **Medications:** Information about different medications, including indications, dosage, side effects, and contraindications.\n",
    "\n",
    "- **Preventive Measures:** Guidelines for maintaining health and preventing diseases, including lifestyle recommendations and screenings.\n",
    "\n",
    "- **Medical Terminology:** Definitions and explanations of complex medical terms to facilitate better understanding for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob = \"*.pdf\",\n",
    "                             loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1\n",
      "\n",
      "\n",
      "\n",
      "Document 2\n",
      "TheGALE\n",
      "ENCYCLOPEDIA\n",
      "ofMEDICINE\n",
      "SECOND EDITION\n",
      "\n",
      "\n",
      "Document 3\n",
      "TheGALE\n",
      "ENCYCLOPEDIA\n",
      "ofMEDICINE\n",
      "SECOND EDITION\n",
      "JACQUELINE L. LONGE, EDITOR\n",
      "DEIRDRE S. BLANCHFIELD, ASSOCIATE EDITOR\n",
      "VOLUME\n",
      "A-B1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(extracted_data[:3]):\n",
    "    print(f\"Document {i+1}\")\n",
    "    print(doc.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 words of the first document: ['TheGALE', 'ENCYCLOPEDIA', 'ofMEDICINE', 'SECOND', 'EDITION']\n"
     ]
    }
   ],
   "source": [
    "# View the first 5 words of the first document in extracted_data\n",
    "first_doc_words = extracted_data[1].page_content.split()[:5]\n",
    "print(\"First 5 words of the first document:\", first_doc_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of my chunk: 7020\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denok\\AppData\\Local\\Temp\\ipykernel_4008\\1337643473.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "e:\\projects\\MedicineChatbot\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index: medical-chatbot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Pinecone API key from the .env file\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize Pinecone with the API key\n",
    "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Name of the index\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "# List existing indexes and check if the index exists\n",
    "existing_indexes = pinecone_client.list_indexes().names()\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    raise ValueError(f\"Index '{index_name}' does not exist. Please create the index first.\")\n",
    "\n",
    "# Connect to the existing index\n",
    "index = pinecone_client.Index(index_name)\n",
    "\n",
    "# Now proceed with creating embeddings for the text chunks\n",
    "docsearch = PineconeVectorStore.from_texts(\n",
    "    [t.page_content for t in text_chunks],\n",
    "    embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(f\"Connected to Pinecone index: {index_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [Document(metadata={}, page_content=\"GALE ENCYCLOPEDIA OF MEDICINE 2 117Allergies\\nAllergic rhinitis is commonly triggered by\\nexposure to household dust, animal fur,or pollen. The foreign substance thattriggers an allergic reaction is calledan allergen.\\nThe presence of an allergen causes the\\nbody's lymphocytes to begin producingIgE antibodies. The lymphocytes of an allergy sufferer produce an unusuallylarge amount of IgE.\\nIgE molecules attach to mast\\ncells, which contain histamine.HistaminePollen grains\\nLymphocyte\\nFIRST EXPOSURE\"), Document(metadata={}, page_content=\"GALE ENCYCLOPEDIA OF MEDICINE 2 117Allergies\\nAllergic rhinitis is commonly triggered by\\nexposure to household dust, animal fur,or pollen. The foreign substance thattriggers an allergic reaction is calledan allergen.\\nThe presence of an allergen causes the\\nbody's lymphocytes to begin producingIgE antibodies. The lymphocytes of an allergy sufferer produce an unusuallylarge amount of IgE.\\nIgE molecules attach to mast\\ncells, which contain histamine.HistaminePollen grains\\nLymphocyte\\nFIRST EXPOSURE\"), Document(metadata={}, page_content=\"GALE ENCYCLOPEDIA OF MEDICINE 2 117Allergies\\nAllergic rhinitis is commonly triggered by\\nexposure to household dust, animal fur,or pollen. The foreign substance thattriggers an allergic reaction is calledan allergen.\\nThe presence of an allergen causes the\\nbody's lymphocytes to begin producingIgE antibodies. The lymphocytes of an allergy sufferer produce an unusuallylarge amount of IgE.\\nIgE molecules attach to mast\\ncells, which contain histamine.HistaminePollen grains\\nLymphocyte\\nFIRST EXPOSURE\")]\n"
     ]
    }
   ],
   "source": [
    "# Connect to the existing Pinecone index and load it using LangChain's PineconeVectorStore\n",
    "docsearch = PineconeVectorStore.from_existing_index(index_name, embeddings)\n",
    "\n",
    "# Query the index\n",
    "query = \"What are Allergies\"\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "\n",
    "# Print the search results\n",
    "print(\"Results:\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\projects\\MedicineChatbot\\research\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"../model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     user_input = input(f\"Input Prompt: \")\n",
    "#     if user_input.lower() == 'exit':\n",
    "#         break\n",
    "#     result = qa.invoke({\"query\": user_input})  # Use invoke instead of calling the object directly\n",
    "#     print(\"Response: \", result[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
